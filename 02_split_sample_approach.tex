\section{The split-sample approach}

    \subsection{Pseudo Likelihood}
    \begin{frame}
        \frametitle{\insertsection\ $-$ \insertsubsection}
        \begin{itemize}
            \item Consider the log-likelihood function $\ell(\boldsymbol{\theta}) = \sum_i \ell(\boldsymbol{y_i}|\boldsymbol{\theta})$ where $\boldsymbol{y_i}$ is the vector of all observations in group $i$
            \item Replaces the log-likelihood contribution $\ell(\boldsymbol{y_i}|\boldsymbol{\theta})$ by a weighted sum of log-likelihood contributions for sub-vectors $\boldsymbol{Y_i}^{(s)}$
            \item More specifically, the pseudo-log-likelihood function:
            \[
                p\ell(\boldsymbol{\psi}) = \sum_i \sum_s \delta_s\ \ell(\boldsymbol{y_i}^{(s)}|\boldsymbol{\psi})
            \]
            is maximized instead with respect to $\boldsymbol{\psi}$, which is not necessarily identical to $\boldsymbol{\theta}$
            \item Although $\hat{\boldsymbol{\psi}}$ is not the MLE estimate, it still has similar properties such as consistency and asymptotic normality \footcite{clark2019}
        \end{itemize}
    \end{frame}

    \note{
        \begin{itemize}
            \item Now, how do we split $\boldsymbol{y_i}$ into sub-vectors $\boldsymbol{Y_i}^{(s)}$? There are different ways to do this, and we will discuss some of them in the next slides.
            \item Going back to the slide on ``issues with large datasets'', it's obvious that we can split the data in two (technically three) different ways: either we can split the data by groups, or we can split the data by observations within groups. The first one is more suitable when we have a large number of groups, while the second one is more suitable when we have a large number of observations within groups.
        \end{itemize}
    }

    \subsection{Graphical representation}
    \begin{frame}
        \frametitle{\insertsection\ $-$ \insertsubsection}
        \begin{figure}
            \centering
            \includegraphics[width=0.73\textwidth]{split_image.png}
            \caption{Graphical representation of different ways to split large samples}
            \label{fig:1}
        \end{figure}
    \end{frame}

    \subsection{Independent subsamples}
    \begin{frame}
        \frametitle{\insertsection\ $-$ \insertsubsection}
        \begin{itemize}
            \item Shown in panel (b) of Figure~\ref{fig:1}, dataset with large $N$ is partitioned into $M$ independent sets $S_m$ of groups, where $m = 1, \ldots, M$
            \item In each subsample, the model is fitted, yielding an estimate $\hat{\boldsymbol{\theta}}_m$ of $\boldsymbol{\theta}$, equivalent to maximizing
            \[
                p\ell(\boldsymbol{\psi}) = \sum_m \sum_{i \in S_m} \ell(\boldsymbol{Y_i}|\boldsymbol{\theta}_m)
            \]
            with respect to $\boldsymbol{\psi} = \{\boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_M\}$
            \item All $\boldsymbol{\theta}_m$ are equal to $\boldsymbol{\theta}$, therefore the estimates $\hat{\boldsymbol{\theta}}_m$ can be averaged to obtain an overall estimate $\hat{\boldsymbol{\theta}}$
        \end{itemize}
    \end{frame}

    \note{
        \begin{itemize}
            \item $\boldsymbol{\theta}_m$ are all equal to $\boldsymbol{\theta}$ because the subsamples are independent
            \item Mention parallelization here, since we can fit the model on each subsample in parallel, which can significantly reduce the computational time.
        \end{itemize}
    }

    \subsection{Dependent subsamples}
    \begin{frame}
        \frametitle{\insertsection\ $-$ \insertsubsection}
        \begin{itemize}
            \item Shown in panel (c) of Figure~\ref{fig:1}, dataset with large $n$ is partitioned into $M$ (not independent) sets $S_m$ of groups, where $m = 1, \ldots, M$
            \item Fitting the model on each subsample, equivalent to maximizing
            \[
                p\ell(\boldsymbol{\psi}) = \sum_m \sum_{i} \ell(\boldsymbol{Y_i}^{(m)}|\boldsymbol{\theta}_m)
            \]
            with respect to $\boldsymbol{\psi} = \{\boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_M\}$, where $\boldsymbol{Y_i}^{(m)}$ is the observations in $\boldsymbol{Y_i}$ belonging to subsample $S_m$.
            \item All $\boldsymbol{\theta}_m$ are not necessarily equal to $\boldsymbol{\theta}$, therefore the combination of all $\hat{\boldsymbol{\theta}}_m$ into a single estimator $\hat{\boldsymbol{\theta}}$ depends on the precise model and data structure.
        \end{itemize}
    \end{frame}

    \note{
        \begin{itemize}
            \item $\boldsymbol{\theta}_m$ are not necessarily equal to $\boldsymbol{\theta}$ because the subsamples are not independent, and there may be some correlation between the observations in different subsamples.
            \item (GPT warning, dont trust 100\%) The combination of all $\hat{\boldsymbol{\theta}}_m$ into a single estimator $\hat{\boldsymbol{\theta}}$ can be done using various methods, such as meta-analysis techniques, or by fitting a model to the estimates $\hat{\boldsymbol{\theta}}_m$ themselves.
        \end{itemize}
    }

    \subsection{Overlapping subsamples}
    \begin{frame}
        \frametitle{\insertsection\ $-$ \insertsubsection}
        \begin{itemize}
            \item Shown in panel (d) of Figure~\ref{fig:1}, longitudinal dataset with very large $n$ is partitioned similarly to dependent subsamples, but association between longitudinal observations is accounted for by letting the subsamples overlap
            \item Denoting the parameters in pair $\{\boldsymbol{Y_i}^{(p)}, \boldsymbol{Y_i}^{(q)}\}$ by $\boldsymbol{\theta}_{p,q}$, fitting the models on all pairs is equivalent to maximizing
            \[
                p\ell(\boldsymbol{\psi}) = \sum_{p < q} \sum_{i} \ell(\boldsymbol{Y_i}^{(p)}, \boldsymbol{Y_i}^{(q)}|\boldsymbol{\theta}_{p,q})
            \]
            with respect to $\boldsymbol{\psi} = \{\boldsymbol{\theta}_{1,2}, \boldsymbol{\theta}_{1,3}, \ldots, \boldsymbol{\theta}_{Q-1,Q} \}$, where $\boldsymbol{Y_i}^{(p)}$ and $\boldsymbol{Y_i}^{(q)}$ are the observations in $\boldsymbol{Y_i}$ belonging to subsamples $S_p$ and $S_q$, respectively.
            \item Similarly, the combination of all $\hat{\boldsymbol{\theta}}_{p,q}$ into a single estimator $\hat{\boldsymbol{\theta}}$ depends on the precise model and data structure.
        \end{itemize}
    \end{frame}

    \note{
        \begin{itemize}
            \item without the pairwise fitting, we have to fit the model on the entire dataset, which is computationally infeasible when $n$ is very large. By fitting the model on pairs of subsamples, we can reduce the computational burden while still accounting for the association between longitudinal observations.
            \item not gonna go into details here since this is more suitable for longitudinal data, which is not the focus of our presentation, but the idea is similar to dependent subsamples
            \item similarly, $\boldsymbol{\psi} = \{\boldsymbol{\theta}_{1,2}, \boldsymbol{\theta}_{1,3}, \ldots, \boldsymbol{\theta}_{Q-1,Q} \} = \{\boldsymbol{\theta}_{p,q} : p < q\}$
            \item what if both $n$ and $N$ are large? there is no mention of this in the literature, should we mention this?
        \end{itemize}
    }