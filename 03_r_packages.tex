\section{R Packages}

    \subsection{\texttt{lme4} and \texttt{mgcv}}
    \begin{frame}
        \frametitle{\insertsection\ $-$ \insertsubsection}
        \texttt{lme4}
        \begin{itemize}
            \item an R package for fitting linear and generalize linear mixed-effects (multilevel) models \footcite{lme4}
            \item efficient, able to handle large sample sizes for simple model, and process hundreds of thousands observations on a typical laptop
            \item Modeling functions: \texttt{lmer()} and \texttt{glmer()}
        \end{itemize}

        \texttt{mgcv}
        \begin{itemize}
            \item an R package for fitting generalized additive model and generalized additive mixed models \footcite{mgcv}
            \item Modeling functions: \texttt{gam()} and \texttt{bam()}
        \end{itemize}
    \end{frame}

    \note{
        \tiny
        can start by saying "in practice, optimization on large datasets are much more complex than what we have shown here, and a short presentation is not enough to cover all the details. we have two packages (used for large datasets) that we want to cover here, that is lme4 and mgcv"

        lme4:
        \begin{itemize}
            \item is an R package for fitting linear and generalized linear mixed-effects (multilevel) models using `Eigen' C\texttt{++} library and S4 classes. (or just say using C++ library). Eigen is a high-level C++ template library for linear algebra that provides efficient, header-only classes for managing matrices, vectors, and numerical solvers. S4 is a formal system in R for object-oriented programming that uses strictly defined classes and methods to ensure data integrity and facilitate complex statistical modeling.
            \item It's computationally efficient, enabling it to handle very large sample sizes for simpler mixed models and to process hundreds of thousands of observations with random effects on a typical laptop.
            \item \texttt{lmer()}: fits linear multilevel model using restricted maximum likelihood (REML) or maximum likelihood estimation. \texttt{glmer()}: fits generalized linear multilevel model, accommodating non-normal response distributions. basically, \texttt{lmer()} for linear models and \texttt{glmer()} for GLM
        \end{itemize}

        mgcv:
        \begin{itemize}
            \item is an R package for fitting generalized additive models (GAMs) and generalized additive mixed models (GAMMs) using penalized regression splines.
            \item \texttt{gam()}: fits generalized additive multilevel models using penalized regression splines with smooth terms that can incorporate multilevel structure through random effect splines. \texttt{bam()}: a computationally efficient version of \texttt{gam()} optimized for very large datasets.
        \end{itemize}
    }

    \subsection{Why use \texttt{bam()}?}
    \begin{frame}
        \frametitle{\insertsection\ $-$ \insertsubsection}
        \begin{itemize}
            \item Same underlying model between \texttt{gam()} and \texttt{lme4}, with differences in parameter estimation
            \item How \texttt{bam()} works:
            \begin{itemize}
                \item QR decomposition \footcite{wood2015}
                \item (i) Efficient fitting algorithm, (ii) Parallel computation, and (iii) Covariate discretization \footcite{wood2017}
                \item Efficient crossproduct matrix $X^\top WX$ computation \footcite{wood2020}
            \end{itemize}
            \item Discretization on large datasets leads to tradeoff between accuracy and speed
        \end{itemize}
    \end{frame}
        
    \note{
        \begin{itemize}
            \item The underlying model between \texttt{gam()} function and \texttt{lme4} is the same, with differences only in the way parameters are estimated.
            \item \texttt{bam()} employs parallelized computation on model matrix subsets and optional data discretization to extract minimal necessary information, enabling efficient estimation of large multilevel models.
            \item Discretization has negligible impact on parameter estimates (differing only at high decimal precision), but leads to dramatic speed improvements.
        \end{itemize}
    }

    \note{
        \tiny
        QR decomposition
        \begin{itemize}
            \item QR decomposition is a method for decomposing a matrix into a lower triangular matrix and an upper triangular matrix.
            \item Fitting GAM $\hat{\beta} = (X^T X + \sum \lambda_j S_j)^{-1}X^T y$ becomes $\hat{\beta} = (R^T R + \sum \lambda_j S_j)^{-1}R^T R$ where $X = QR$, $X$ is the design matrix, $S_j$ is the penalty matrix, and $\lambda_j$ are smoothing parameters.
        \end{itemize}
        (i) Efficient fitting algorithm, (ii) Parallel computation, and (iii) Covariate discretization
        \begin{itemize}
            \item (i) Efficient fitting algorithm: which required only basic easily parallelized matrix computations and a pivoted Cholesky decompostion
            \item (ii) Parallel computation: the use of a scalable parallel block pivoted Cholesky algorithm mentioned above
            \item (iii) Covariate discretization: an efficient approach to model matrix storage and computations with the model matrix, using discretized covariates. For example, there are only a finite number of site locations, site labels and elevations, temperature is only recorded to within $0.1^\circ C$ (or any precision), etc
            \item These three elements work together, and dropping any one of them leads to an increase in fitting time of an order of magnitude or more. also, mention that this is a new algorithm that is better than QR decomposition. 3 orders of magnitude faster than QR decomposition
        \end{itemize}
        Efficient crossproduct matrix $X^\top WX$ computation
        \begin{itemize}
            \item the most expensive part of previous algorithm is the formation of the matrix crossproduct. this approach present a simple, novel and substantially more efficient approach to the computation of this cross product
        \end{itemize}
    }

    \subsection{When to use \texttt{bam()}?}
    \begin{frame}
        \frametitle{\insertsection\ $-$ \insertsubsection}
        \begin{itemize}
            \item In general, \texttt{lme4} is preferred due to easy syntax and robust estimation
            \item \texttt{bam()} is particularly useful for:
            \begin{itemize}
                \item Complex models that exceed \texttt{lme4}'s capabilities
                \item Incorporating smooth (nonlinear) terms
                \item Large datasets with memory issues
                \item Leveraging parallel computing resources
            \end{itemize}
        \end{itemize}
    \end{frame}

    \note{
        \begin{itemize}
            \item In general, \texttt{lme4} is preferred for most multilevel datasets due to its straightforward syntax and robust estimation methods.
            \item \texttt{bam()} is particularly useful when:
            \begin{itemize}
                \item You have complicated structure that begins to bog down lme4
                \item You want to add smooth terms1
                \item You have memory issues
                \item You have a computing setup that can take advantage of bam
            \end{itemize}
        \end{itemize}
    }